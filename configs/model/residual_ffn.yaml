# # configs/model/residual_network.yaml
# in_features: 2
# out_features: 1
# hidden_dim: 64
# # Either specify num_blocks OR (optionally) num_layers like in FFN
# num_blocks: 4          # x2 linear layers inside residual trunk
# layers_per_block: 3    # 2 or 3
# # num_layers: 14       # (optional) converts to ~6 residual blocks

# activation: tanh       # relu/gelu/tanh/sine...
# dropout: 0.0
# norm: none             # or "layernorm"
# pre_norm: trues
# res_scale: 1.0

# use_fourier_features: true
# fourier_scale: 1.0
# fourier_dim: 128

# patch:
#   x: None
#   y: None

model_name: residual_ffn
in_features: 2        # or 3 for (x,y,t)
out_features: 1
hidden: 128
depth: 6
layers_per_block: 2   # <- your knob
activation: gelu
dropout: 0.0
use_layernorm: false
residual_scale: 1.0
input_stem: 1
output_layers: 0