name: patch_attention
in_features: 2
out_features: 1

patch:
  x: 3         # 3 points in x per patch
  y: 3         # 3 points in y per patch (total 9 points)

activation: relu
hidden_dim: 32   # Original size as intended
num_layers: 3
num_heads: 4
ff_mult: 4

# optional
attn_dropout: 0.0
resid_dropout: 0.0
prenorm: true
use_relpos_bias: true
relpos_hidden: 32
normalize_patch_coords: true