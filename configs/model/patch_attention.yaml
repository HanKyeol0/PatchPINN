name: patch_attention
in_features: 2
out_features: 1

patch:
  x: 5         # 8 points in x per patch
  y: 5         # 8 points in y per patch (total 64 points)

activation: gelu   # Better for transformers
hidden_dim: 64    # Larger model
num_layers: 4
num_heads: 4
ff_mult: 4

pos_encoding: sinusoidal  # Options: "fourier", "sinusoidal", "learned", "none"
fourier_scale: 5.0

# Regularization
attn_dropout: 0.05
resid_dropout: 0.05
prenorm: true
use_relpos_bias: true
relpos_hidden: 64
normalize_patch_coords: true