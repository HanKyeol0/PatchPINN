# configs/model/patch_attention.yaml
name: patch_attention
in_features: 2
out_features: 1

patch:
  x: 3         # cells in x (points per patch in x = 21)
  y: 3          # cells in y (points per patch in y = 17)

activation: relu # or relu, tanh, sine (supported via get_act)
hidden_dim: 32
num_layers: 3
num_heads: 4
ff_mult: 4

# optional but useful
attn_dropout: 0.0
resid_dropout: 0.0
prenorm: true
use_relpos_bias: true
relpos_hidden: 32
normalize_patch_coords: true
