name: patch_attention
in_features: 2
out_features: 1

patch:
  x: 5         # 8 points in x per patch
  y: 5         # 8 points in y per patch (total 64 points)

activation: gelu   # Better for transformers
hidden_dim: 64    # Larger model
num_layers: 4
num_heads: 4
ff_mult: 4

# Regularization
attn_dropout: 0.1
resid_dropout: 0.1
prenorm: true
use_relpos_bias: true
relpos_hidden: 64
normalize_patch_coords: true