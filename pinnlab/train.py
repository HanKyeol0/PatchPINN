import os, time, yaml, argparse, sys
import torch
import wandb
import csv
from tqdm import trange
from collections import deque
from pinnlab.registry import get_model, get_experiment
from pinnlab.utils.seed import seed_everything
from pinnlab.utils.early_stopping import EarlyStopping
from pinnlab.utils.plotting import save_plots_1d, save_plots_2d
from pinnlab.utils.wandb_utils import setup_wandb, wandb_log, wandb_finish
from pinnlab.utils.gradflow import GradientFlowLogger
from pinnlab.utils.loss_balancer import BalancerConfig, make_loss_balancer
from pinnlab.utils.plotting import plot_weights_over_time

def load_yaml(path):
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)

def _save_yaml(path, obj):
    with open(path, "w", encoding="utf-8") as f:
        yaml.safe_dump(obj, f)

def main(args):
    base_cfg = load_yaml(args.common_config)
    model_cfg = load_yaml(args.model_config)
    exp_cfg   = load_yaml(args.exp_config)

    # Allow experiment to override in/out dims if needed
    in_features  = exp_cfg.get("in_features", model_cfg.get("in_features"))
    out_features = exp_cfg.get("out_features", model_cfg.get("out_features"))
    model_cfg["in_features"]  = in_features
    model_cfg["out_features"] = out_features
    model_cfg["patch"]["x"] = exp_cfg.get("patch", {}).get("x", None)
    model_cfg["patch"]["y"] = exp_cfg.get("patch", {}).get("y", None)
    model_cfg["patch"]["t"] = exp_cfg.get("patch", {}).get("t", None)

    microbatches       = int(base_cfg["train"].get("microbatches", 16))
    patches_per_batch  = int(base_cfg["train"].get("patches_per_batch", 32))  # number of patches per micro-step
    
    accumulate_steps   = int(base_cfg["train"].get("accumulate_steps", 1))
    assert accumulate_steps >= 1
    
    seed_everything(base_cfg["seed"])

    if exp_cfg.get("device"):
        base_cfg["device"] = exp_cfg["device"]
    device = torch.device(base_cfg["device"] if torch.cuda.is_available() else "cpu")
    exp = get_experiment(args.experiment_name)(exp_cfg, device)
    model = get_model(args.model_name)(model_cfg).to(device)
    tag = exp_cfg.get("tag", None)

    if tag:
        file_name = f"{args.experiment_name}_{args.model_name}_{tag}"
    else:
        ts = time.strftime("%Y%m%d-%H%M%S")
        file_name = f"{args.experiment_name}_{args.model_name}_{ts}"

    torch.cuda.reset_peak_memory_stats(device)

    # Logging dir
    out_dir = os.path.join(base_cfg["log"]["out_dir"], args.experiment_name, file_name)
    os.makedirs(out_dir, exist_ok=True)

    _save_yaml(os.path.join(out_dir, "config.yaml"), {
        "base": base_cfg, "model": model_cfg, "experiment": exp_cfg
    })
    
    # Loss balancer
    use_loss_balancer = base_cfg["train"]["loss_balancer"].get("use_loss_balancer", False)
    if use_loss_balancer:
        lb_cfg_dict = base_cfg["train"].get("loss_balancer", {})  # {'kind': 'dwa', 'terms': ['res','bc','ic'], ...}
        lb_cfg = BalancerConfig(**lb_cfg_dict)
        if not lb_cfg.terms:
            lb_cfg.terms = ["res", "bc", "ic"]   # tailor per experiment

        balancer = make_loss_balancer(lb_cfg)

    weights_csv = os.path.join(out_dir, "loss_weights.csv")
    weights_terms = None  # will infer on first log

    # Optimizer
    params = list(model.parameters())
    if use_loss_balancer:
       params += list(balancer.extra_params())  # no-op for other schemes

    opt_cfg = base_cfg["train"]["optimizer"]
    if opt_cfg["name"].lower() == "adam":
        optimizer = torch.optim.Adam(params, lr=opt_cfg["lr"], weight_decay=opt_cfg.get("weight_decay", 0.0))
    else:
        raise ValueError("Only Adam is wired in, add more in train.py.")

    # create file with header late (when we know the terms)
    def _ensure_weights_header(terms):
        nonlocal weights_terms
        if weights_terms is None:
            weights_terms = list(terms)
            with open(weights_csv, "w", newline="") as f:
                writer = csv.writer(f)
                writer.writerow(["step"] + weights_terms)

    # WandB
    if base_cfg["log"]["wandb"]["enabled"]:
        wandb.login(key=os.getenv('WANDB_API_KEY'))
        wandb.init(project = base_cfg["log"]["wandb"]["project"],
                   name = file_name)
        run = setup_wandb(base_cfg["log"]["wandb"], args, out_dir, config={
            "base": base_cfg, "model": model_cfg, "experiment": exp_cfg
        })

    # gradient flow logger
    use_gradflow = base_cfg["gradflow"]["enabled"]
    if use_gradflow:
        gf = GradientFlowLogger(
            model, out_dir,
            enable=base_cfg.get("gradflow", {}).get("enabled", True),
            every=int(base_cfg.get("gradflow", {}).get("every", 1000)),
            store_vectors=bool(base_cfg.get("gradflow", {}).get("store_vectors", False)),
            max_keep_per_layer=int(base_cfg.get("gradflow", {}).get("max_keep", 20)),
            wandb_hist=bool(base_cfg.get("gradflow", {}).get("wandb_hist", False)),
            plot_cfg=base_cfg.get("gradflow", {}).get("plot", {})
        )
        gf_stop = base_cfg["gradflow"]["stop_at"]

    epochs = base_cfg["train"]["epochs"]
    eval_every = int(base_cfg.get("eval").get("every", 100))

    # Early stopping
    es_cfg = base_cfg["train"]["early_stopping"]
    early = EarlyStopping(patience=es_cfg["patience"], min_delta=es_cfg["min_delta"], eval_every=eval_every) if es_cfg["enabled"] else None
    best_state = None
    best_metric = float("inf")

    # Training loop
    w_res = base_cfg["train"]["loss_weights"]["res"]
    w_bc = base_cfg["train"]["loss_weights"]["bc"]
    w_ic = base_cfg["train"]["loss_weights"]["ic"]

    use_tty = sys.stdout.isatty()
    pbar = trange(
        epochs,
        desc="Training",
        ncols=120,
        dynamic_ncols=True,
        leave=False,          # don't leave old bars behind
        disable=not use_tty,  # if output is piped, avoid multiline spam
    )

    print("training started")
    global_step = 1

    training_start_time = time.time()

    last_iter_time = training_start_time
    iter_time_accum = 0.0
    iter_count = 0

    for ep in pbar:
        model.train()
        
        exp.prepare_epoch_patch_bank()

        running = {"res": 0.0, "bc": 0.0, "ic": 0.0, "total": 0.0}
        
        for mb in range(microbatches):
            # ---- fetch a small subset of patches
            patches = exp.sample_minibatch(patches_per_batch, shuffle=False)

            # ---- compute losses on the minibatch
            loss_res = exp.pde_residual_loss(model, patches, ep)
            loss_bc  = exp.boundary_loss(model, patches)
            loss_ic  = exp.initial_loss(model, patches)

            # scalarize
            loss_res_s = loss_res.mean() if torch.is_tensor(loss_res) and loss_res.dim() > 0 else loss_res
            loss_bc_s  = loss_bc.mean()  if torch.is_tensor(loss_bc)  and loss_bc.dim()  > 0 else loss_bc
            loss_ic_s  = loss_ic.mean()  if torch.is_tensor(loss_ic)  and loss_ic.dim()  > 0 else loss_ic

            if use_gradflow and global_step % gf.every == 0:
                if ep <= gf_stop:
                    gf.collect({"res": loss_res_s, "bc": loss_bc_s, "ic": loss_ic_s}, step=global_step)

            losses = {"res": loss_res_s, "bc": loss_bc_s, "ic": loss_ic_s}

            if not use_loss_balancer:
                total_loss = w_res*loss_res_s + w_bc*loss_bc_s + w_ic*loss_ic_s
                s = (w_res + w_bc + w_ic) or 1.0
                w_now = {"res": w_res/s, "bc": w_bc/s, "ic": w_ic/s}
            else:
                total_loss, w_dict, aux = balancer(losses, step=global_step, model=model)
                w_now = {k.split("/", 1)[1]: float(v) for k, v in w_dict.items()}

            # write weights row on first microbatch of the epoch (keeps CSV compact)
            if mb == 0:
                _ensure_weights_header(w_now.keys())
                with open(weights_csv, "a", newline="") as f:
                    writer = csv.writer(f)
                    writer.writerow([global_step] + [w_now[t] for t in weights_terms])

            # ---- backward / step (with optional accumulation)
            (total_loss / accumulate_steps).backward()
            if (mb + 1) % accumulate_steps == 0:
                optimizer.step()
                optimizer.zero_grad(set_to_none=True)

            # accumulate stats
            running["res"]   += float(loss_res_s.detach().cpu())
            running["bc"]    += float(loss_bc_s.detach().cpu())
            running["ic"]    += float(loss_ic_s.detach().cpu())
            running["total"] += float(total_loss.detach().cpu())

            # per-microbatch logging
            it_per_sec = pbar.format_dict.get("rate", None)
            elapsed_s  = pbar.format_dict.get("elapsed", None)
            gpu_now = {
                "gpu/mem_alloc_mb": float(torch.cuda.memory_allocated(device)) / (1024**2),
                "gpu/mem_reserved_mb": float(torch.cuda.memory_reserved(device)) / (1024**2),
            }
            log_payload = {
                "loss/total": float(total_loss.detach().cpu()),
                "loss/res": float(loss_res_s.detach().cpu()),
                "loss/bc": float(loss_bc_s.detach().cpu()),
                "loss/ic": float(loss_ic_s.detach().cpu()),
                "lr": optimizer.param_groups[0]["lr"],
                "epoch": ep,
                "perf/it_per_sec_tqdm": it_per_sec if it_per_sec is not None else 0.0,
                "perf/elapsed_sec": elapsed_s if elapsed_s is not None else 0.0,
                **gpu_now,
            }
            wandb_log(log_payload, step=global_step, commit=True)
            pbar.set_postfix({k: f"{v:.3e}" for k, v in log_payload.items() if "loss" in k})

            global_step += 1
            
        # Simple validation metric (relative L2 on a fixed grid)
        best_path = os.path.join(out_dir, "best.pt")
        if (ep % eval_every == 0) or (ep == epochs - 1):
            with torch.no_grad():
                rel_l2 = exp.relative_l2_on_grid(model, base_cfg["eval"]["grid"])
            wandb_log({"eval/rel_l2": rel_l2, "epoch": ep}, step=global_step)

            best_path = os.path.join(out_dir, "best.pt")
            if rel_l2 < (best_metric - es_cfg.get("min_delta", 0.0)):
                best_metric = rel_l2
                best_state = {k: v.clone() for k, v in model.state_dict().items()}
                torch.save({k: v.detach().cpu() for k, v in best_state.items()}, best_path)
            if early and early.step(rel_l2):
                print(f"\n[EarlyStopping] Stopping at epoch {ep}. Best rel_l2={best_metric:.3e}")
                break
            
    training_end_time = time.time()

    final_perf = {
        "perf/total_time_sec": training_end_time - training_start_time,
        "gpu/peak_mem_alloc_mb": float(torch.cuda.max_memory_allocated(device)) / (1024**2),
        "gpu/peak_mem_reserved_mb": float(torch.cuda.max_memory_reserved(device)) / (1024**2),
    }

    if exp_cfg.get("video", {}).get("enabled", False):
        vid_grid = exp_cfg.get("video", {}).get("grid", {"x": exp_cfg["grid"]["x"], "y": exp_cfg["grid"]["y"]})
        nt_video = exp_cfg.get("video", {}).get("nt", exp_cfg["grid"]["t"])
        fps      = exp_cfg.get("video", {}).get("fps", 10)
        out_fmt  = exp_cfg.get("video", {}).get("format", "mp4")  # "mp4" or "gif"
        vid_path = exp.make_video(
            model, vid_grid, out_dir,
            nt_video=nt_video, fps=fps,
        )
        wandb_log({"video/evolution": wandb.Video(vid_path, format=out_fmt)}, step=global_step)

    wandb_log(final_perf, step=global_step)

    weights_png = os.path.join(out_dir, "loss_weights.png")
    plot_weights_over_time(weights_csv, weights_png)
    print(f"[weights] saved: {weights_csv}")
    print(f"[weights] plot : {weights_png}")

    if use_gradflow:
        gf.save()
        if gf.plot_enabled:
            saved = gf.save_plots()
            for p in saved:
                print(f"[gradflow] saved plot: {p}")

    # Restore best
    if best_state:
        model.load_state_dict(best_state)

    # Final evaluation & plots
    model.eval()
    figs = exp.plot_final(model, base_cfg["eval"]["grid"], out_dir)
    for name, path in figs.items():
        wandb_log({f"fig/{name}": wandb.Image(path)}, step=global_step)

    wandb_finish()
    print(f"Artifacts saved to: {out_dir}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_name", required=True)
    parser.add_argument("--experiment_name", required=True)
    parser.add_argument("--common_config", required=True)
    parser.add_argument("--model_config", required=True)
    parser.add_argument("--exp_config", required=True)
    args = parser.parse_args()
    main(args)